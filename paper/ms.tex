%%
%% Beginning of file 'sample62.tex'
%%
%% Modified 2018 January
%%
%% This is a sample manuscript marked up using the
%% AASTeX v6.2 LaTeX 2e macros.
%%
%% AASTeX is now based on Alexey Vikhlinin's emulateapj.cls 
%% (Copyright 2000-2015).  See the classfile for details.

%% AASTeX requires revtex4-1.cls (http://publish.aps.org/revtex4/) and
%% other external packages (latexsym, graphicx, amssymb, longtable, and epsf).
%% All of these external packages should already be present in the modern TeX 
%% distributions.  If not they can also be obtained at www.ctan.org.

%% The first piece of markup in an AASTeX v6.x document is the \documentclass
%% command. LaTeX will ignore any data that comes before this command. The 
%% documentclass can take an optional argument to modify the output style.
%% The command below calls the preprint style  which will produce a tightly 
%% typeset, one-column, single-spaced document.  It is the default and thus
%% does not need to be explicitly stated.
%%
%%
%% using aastex version 6.2
\documentclass[twocolumn]{aastex62}
\usepackage{amsmath, amssymb, mathtools, bm}

\newcommand{\KTWO}{$\mathcal{K}\mathit{2}$}
\newcommand{\tess}{TESS}
\newcommand{\lightkurve}{\texttt{lightkurve}}
\newcommand{\LightCurve}{\texttt{LightCurve}}
\newcommand{\KeplerLightCurve}{\texttt{KeplerLightCurve}}
\newcommand{\numpy}{\texttt{numpy}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\vdag}{(v)^\dagger}
\newcommand\aastex{AAS\TeX}
\newcommand\latex{La\TeX}

%% Reintroduced the \received and \accepted commands from AASTeX v5.2
\received{January 1, 2018}
\revised{January 7, 2018}
\accepted{\today}
%% Command to document which AAS Journal the manuscript was submitted to.
%% Adds "Submitted to " the arguement.
\submitjournal{ApJ}

%\shorttitle{Sample article}
%\shortauthors{Schwarz et al.}

\begin{document}

\title{\lightkurve: a Python package for Kepler \& TESS time series data analysis}

\correspondingauthor{Geert Barentsen}
\email{hello@geert.io}

\author{\lightkurve~contributors}
\affil{The galatic startup}

\author{José Vinícius de Miranda Cardoso}
\affiliation{Bay Area Environmental Research Institute \\
Petaluma \\
California, USA}
\collaboration{(AAS Journals Data Scientists collaboration)}

\begin{abstract}

    This paper presents an open source package for NASA's Kepler, \KTWO, and
    TESS.

\end{abstract}

%% Keywords should appear after the \end{abstract} command.
%% See the online documentation for the full list of available subject
%% keywords and the rules for their use.
\keywords{editorials, notices ---
miscellaneous --- catalogs --- surveys}

\section{Introduction} \label{sec:intro}
NASA's Kepler, \KTWO, and \tess~missions have been delivering high-precision time
series data for a wide range of stellar types.
Cite Borucki et al for Kepler, Howell et al for K2, XXX for TESS.

While these missions have powerful pipelines which deliver high-precision
lightcurves for many objects (citation), analyses tailored for specific
science cases can often optimize the analysis.
For example:
1) a bigger aperture mask has been shown to yield additional planets in Kepler-XXX (cite Isabel),
2) custom tailored PSF photometry has been shown to disentangle the signals of stars in
dense clusters (cite Libralato et al),
3) the official pipelines tend to be optimized for signals at the duration of typical
planet transits (6 hours) and are not optimized for long-term trends such as those
seen in AGN (cite Krista Lynn Smith).

Explain PyKE exists and how it differs.
In this paper, we present a new Python package which makes the custom analysis
of target easy.  Based on AstroPy (cite Astropy).

Explain the structure of the paper and announce that we will show examples
of the 3 key features: manipulating lightcurves, creating lightcurves,
and removing systematics from lightcurves.

\section{Package overview}

Maybe include a class diagram here, or a table of the modules or classes.

Maybe include a table or graph listing the key functions of the key classes,
e.g.
Lightcurve: cdpp, fold, stitch, plot
TargetPixelFile: tolightcurve, plot


\section{Examples of key features}

\subsection{Working with lightcurves}

flatten, fold, finding planets, from archive...

\subsection{Making your own lightcurves from target pixel files}

explain TPFs, aperture photometry, and PSF photometry

\subsection{Removing systematics from lightcurves}

We only intend to provide simple tools.
Ideally, systematics are removed simultaneously with fitting a model (e.g. Montet and DFM 2015).

\section{Future work}

Explain PSF photometry needs users and data-driven model capability.

We do not intend to implement transit fitting, more advanced detrending, etc.
Instead, lightkurve intends to provide the building blocks needed to build
or interact with such packages.

We intend to add many tutorials.

Explain how people can contribute.


\section{Conclusions}

we will discuss => we have discussed


\section{Lightcurve Basics}

\subsection{The \LightCurve~and \KeplerLightCurve~classes}
        The \LightCurve~class is a simple cointainer to store \numpy~arrays
        (hereafter, arrays) related to flux time-domain measurements.

        The \LightCurve~object provides methods to store, process, and
        convert lightcurves. Table~\ref{tab:methods} contains a description
        of a subset of the methods.

        \begin{table}[!htb]
            \centering
            \caption{A subset of methods provided by the \LightCurve~class}
            \begin{tabular}{cp{6.5cm}}
                \hline
                \textbf{Method} & \textbf{Short description} \\
                \hline
                \texttt{stitch} & appends the attributes \texttt{flux},
                \texttt{time}, and \texttt{flux\_err} of other given
                \texttt{LighCurve} objects.\\
                \texttt{flatten} & applies a Savitzky-Golay filter to capture
                low frequency flux variations which can be then removed in order
                to aid transit detection algorithms.\\
                \texttt{fold} & folds a lightcurve at a given period and phase.\\
                \texttt{bin} &  bins a lightcurve using a block mean or median.\\
                \texttt{cdpp} &  computes the Combined Differential Photometric
                Precision (CDPP) metric, which is a proxy for the amount of
                scatter in the lightcurve signal. \\
                \texttt{plot} & displays a lightcurve.
            \end{tabular}
            \label{tab:methods}
        \end{table}

        A \LightCurve~object can be instantiated by passing a \texttt{time}
        array, a \texttt{flux} array, and, optionally, a \texttt{flux\_err} array
        which accounts for uncertainties in the \texttt{flux} measurements, i.e.,
\begin{verbatim}
>>> from lightkurve import LightCurve
>>> lc = LightCurve(time, flux)
\end{verbatim}

       The \KeplerLightCurve class extends \LightCurve by
       adding attributes to store metadata information such as channel number,
       quality flags, campaign or quarter number, kepler id, etc.

       Additionally, \texttt{KeplerLightCurve} can be corrected for motion-dependent
       correlated noise using the \texttt{correct} method which will be discussed in
       Section~\ref{subsection:motion}.

   \subsection{The \texttt{KeplerLightCurveFile} class}
        The \texttt{KeplerLightCurveFile} class defines a structure to deal
        with lightcurve files from both NASA's Kepler and K2 missions.

        To instantiate a \texttt{KeplerLightCurveFile} object, it is necessary
        to pass a \texttt{path} which represents the address (url or local path)
        of a lightcurve file in the fits (or compressed) format, and a
        \texttt{quality\_bitmask} string which specifies quality
        flags of cadences that should be ignored.

        One crucial method of the \texttt{KeplerLightCurveFile} class is
        \texttt{get\_lightcurve} which returns a \texttt{KeplerLightCurve} object
        with the metadata provided by the corresponding \texttt{KeplerLightCurveFile}.

        Therefore, one can, for example, perform the following series of operations
        in order to fold a lightcurve from the MAST archive
\begin{verbatim}
>>> lc_file = KeplerLightCurveFile("kplr011904151-2009350155506_llc.fits")
>>> klc = lc_file.PDCSAP_FLUX.fold(period=0.837495)
>>> klc.plot()
\end{verbatim}

\begin{figure}
\plotone{figs/fold-lc.pdf}
\caption{Folded lightcurve of target \texttt{KIC011904151} quarter 3, showing the
            transit signal of Kepler-10b.
\label{fig:fold-method}}
\end{figure}

\section{Target Pixel File Basics}
    \subsection{The \texttt{KeplerTargetPixelFile} class}
        A \texttt{KeplerTargetPixelFile} object can be instantiated
        by passing a \texttt{path} (URL or local) of a target pixel file.
        Optionally, the user can elect to throw away frames that contain
        a specific flag by using the \texttt{quality\_bitmask} argument.

        \texttt{KeplerTargetPixelFile} offers a number of methods
        that range from getting raw aperture photometry lightcurves to
        data visualization.

        For instance, the method \texttt{plot} can be used to visualize a
        given frame, which are depicted in Fig.~\ref{fig:plot-method}.
\begin{verbatim}
>>> import numpy as np
>>> from lightkurve import KeplerTargetPixelFile
>>> tpf = KeplerTargetPixelFile("kplr008462852-2011073133259_lpd-targ.fits")
>>> tpf.plot()
>>> tpf.plot(aperture_mask=tpf.flux[0] > np.nanmean(tpf.flux[0]))
\end{verbatim}

\begin{figure}[!htb]
    \centering
    \plotone{figs/tpf-plot.pdf}
    \plotone{figs/tpf-plot-aperture.pdf}
    \caption{Displaying a given frame of a TPF using \texttt{plot}.
    Optionally, an \texttt{aperture\_mask} can be passed which is
    highlighted on the right hand side.}
    \label{fig:plot-method}
\end{figure}

In an image with $n$ pixels, where the flux and the center positions of the
$i$-th pixel are denoted as $f_i$ and $(x_i, y_i)$, respectively, the centroids
may be expressed as
\begin{align}
    x^{\star} = \dfrac{\sum_{i=1}^{n} f_i x_i}{\sum_{i=1}^{n}f_i} \\
    y^{\star} = \dfrac{\sum_{i=1}^{n} f_i y_i}{\sum_{i=1}^{n}f_i}.
\end{align}

In \texttt{lightkurve}, the centroids in every cadence can be computed as
\begin{verbatim}
>>> from lightkurve import KeplerTargetPixelFile
>>> tpf = KeplerTargetPixelFile('ktwo246199087-c12_lpd-targ.fits.gz')
>>> x_star, y_star = tpf.get_centroids()
\end{verbatim}


\section{Tools}

\subsection{Cotrending basis vectors}

Cotrending basis vectors (CBVs) can remove global correlated
systematics present in a given channel \cite{smith2012}.

Given a set of $n$ CBVs, one is interested in finding a vector of $n$
coefficients $\bm{\theta}=(\theta_1, \theta_2, ..., \theta_n)$ which minimizes
some cost function between the SAP flux and the set of CBVs. The mathematical
structure of the cost function is a direct consequence of the statistical
assumptions made for data.

For instance, if one assumes that the data comes from an independent and
identically distributed (iid) multivariate Gaussian distribution with mean
$\sum_{j=1}^{n}\theta_j v_{j}(t)$ and known variance $\sigma^2$, then the
cost function can be expressed as follows
\begin{align}
    \mathcal{C}(\bm{\theta}, f_{SAP}) = \sum_{t}\left(f_{SAP}(t)
    - \sum_{j=1}^{n}\theta_j v_{j}(t)\right)^2,
\label{eq:chi-square}
\end{align}
in which $f_{SAP}$ is the SAP flux and $v_j$ is the $j$-th CBV.

The maximum likelihood estimator for $\bm{\theta}$, $\bm{\theta^{\star}}$ can be
expressed as
\begin{align}
    \bm{\theta}^{\star} = \argmin_{\bm{\theta} \in \Theta} \mathcal{C}(\bm{\theta}, f_{SAP}).
\end{align}

However, Equation~(\ref{eq:chi-square}) is sensitive to outliers~\cite{ivezi2014},
therefore, as a default behaviour in \lightkurve, we use the following cost
function
\begin{align}
    \mathcal{C}(\bm{\theta}, f_SAP) = \sum_{t} \left\rvert f_{SAP}(t)
    - \sum_{j=1}^{n}\theta_j v_{j}(t)\right\rvert.
\label{eq:l1-norm}
\end{align}


Then, the CBV-corrected flux can be computed as
\begin{equation}
    f_{CBV} = f_{SAP} - \sum_{j=1}^{n}\theta^{\star}_j v_{j}(t).
\end{equation}

An example of SAP flux correction for target \texttt{KOI 8462852}
(Tabby's star) can be written as follows
\begin{verbatim}
>>> from lightkurve.lightcurve import KeplerCBVCorrector
>>> cbv = KeplerCBVCorrector("kplr008462852-2011073133259_llc.fits")
>>> cbv_lc = cbv.correct(cbvs=[1,2])
\end{verbatim}

Fig~\ref{fig:cbv-correction} illustrates the correction. The pink line has a shift from
the green line because in \lightkurve~we do not account the flux lost outside of the aperture mask.

\begin{figure}[!htb]
    \centering
    \plotone{figs/cbv.pdf}
    \caption{CBV correction applied on \texttt{KOI 8462852}}
    \label{fig:cbv-correction}
\end{figure}


Improperly tuning the number of CBVs can cause over-/under-fitting. One
way to identify a reasonable number of CBVs is to perform a grid search
as shown in Fig~(\ref{fig:cbv-grid-search}).

In the same fashion, we can apply cotrending basis vector correction to
\KTWO lightcurves.

Fig.~\ref{fig:cbv-correction-k2} shows the detrended lightcurve after estimating
the first nine coefficients for CBV correciton on \KTWO target
 \texttt{EPIC 201543306}. The selection of the number of CBVs is set by inspecting the
grid search curve can be set through model comparison heuristics like AIC, BIC,
or cross-validation \cite{ivezi2014}.

\begin{figure}
    \centering
    \plotone{figs/cbv-k2.pdf}
    \caption{CBV correction applied on \texttt{EPIC 201543306}.
    \label{fig:cbv-correction-k2}}
\end{figure}

\subsubsection{Number of CBVs}
The number of CBVs will directly contribute to overfitting effects. One
way to identify a reasonable number of CBVs is to perform a grid search
as suggested in Fig~(\ref{fig:cbv-grid-search}), which shows the cost
function as a function of the number of CBVs. Usually, as the number of
CBVs increases, the value of the cost function decreases. And therefore,
the user should empirically choose a number of CBVs which does not
remove the astrophysical signal of interest [add reference].

\begin{figure}[!htb]
    \centering
    \plotone{figs/cbv-grid-search.pdf}
    \caption{Grid search on the number of CBVs}
    \label{fig:cbv-grid-search}
\end{figure}

% MGS note: Wait--- won't the cost function decrease monotonically for CBVs?
% Do you use AIC or BIC or cross-validation?
An objective way of selecting the number of CBVs is to use Bayes' factors
[add reference]. In the Bayes' factor setting, the selected number of
CBVs is the one that provide the least gain in posterior probability, i.e.,
for all ordered pairs of CBVs, the Bayes factor selects $n^{\star}$ number of CBVs
as follows
\begin{align}
    n^{\star} = \argmin_{n} \dfrac{p_{n+1}}{p_n},
\end{align}
in which $p_n$ is the posterior probability evaluated at the Maximum A Posteriori
Estimator (MAP) obtained using $n$ CBVs.

A Laplacian prior with zero mean and variance $16$ is the default prior
density over the CBVs coefficients.

\begin{figure}[!htb]
    \centering
    \plotone{figs/cbv-bayes-factor.pdf}
    \caption{Bayes factors as a function of the number of CBVs for \texttt{EPIC 201543306}}
    \label{fig:cbv-grid-search}
\end{figure}


\subsection{Point spread function photometry}

\lightkurve ~contains routines to perform PSF photometry in TPFs
which are implemented in the \texttt{psf} module.

Briefly, the PSF photometry problem that \lightkurve~solves can be formulated as
follows. Given an image $\bm{y}$, with $n$ pixels and $m$ stars, and a PSF model
$\lambda(\bm{\theta}) = \sum_{j=1}^{m} \lambda({\theta}_j)$,
find the best parameter vector (which encodes fluxes and center positions for $m$
stars) $\bm{\theta}^{\star} = (\theta_1^{\star}, \theta_2^{\star}, ..., \theta_m^{\star})$
that minimizes some cost (or loss) function $R(\lambda(\bm{\theta}), \bm{y})$
of assigning $\bm{\theta} = \bm{\theta}^{\star}$.

From a probabilistic point of view, one is often interested in minimizing the
expected cost with respect to some probability distribution assigned to the data
$\bm{y}$ and to the parameter vector $\bm{\theta}$, from which the cost function
$R$ naturally arises. The default assumption, made in \lightkurve,~on the data is
that it follows a Poisson probability distribution, whereas the probability
distribution on the parameter vector has to be assigned by the user using the $\texttt{prior}$
argument. Using a uniform prior for $\bm{\theta}$, the MAP estimator can be written
as
\begin{align}
    \bm{\theta}^{\star}(\bm{y}) = \argmin_{\bm{\theta} \in \Lambda} \sum_{i=1}^{n}
    \left(\sum_{j=1}^{m}\lambda_i(\bm{\theta}_j) - y_i\log\sum_{j=1}^{m}\lambda_i(\bm{\theta}_j)\right),
\end{align}
in which $\Lambda$ is the support of $\bm{\theta}$.

The example below illustrates PSF photometry on the target \texttt{EPIC 246199087}
(Trappist-1):

\begin{verbatim}
>>> from lightkurve import KeplerTargetPixelFile
>>> from lightkurve.psf import PRFPhotometry, SceneModel
>>> from oktopus import UniformPrior
>>> tpf = KeplerTargetPixelFile("ktwo246199087-c12_lpd-targ.fits.gz")
>>> prf = tpf.get_prf_model()
>>> prior = UniformPrior(lb=[4e3, 990, 25, 1], ub=[2e4, 996, 30, 2e3])
>>> scene = SceneModel(prf=[prfs])
>>> phot = PRFPhotometry(scene_model=scene, prior=prior)
>>> results = phot.fit(tpf.flux + tpf.flux_bkg)
\end{verbatim}

The photometric results are stored in a $c \times 4$ matrix, where $c$ is the
number of frames (cadences).

Another important aspect is the PSF model...

\subsection{Motion-dependent Correlated Noise}
\label{subsection:motion}

Spacecraft-induced correlated noise remains one of the greatest hurdles to
analyzing K2 lightcurves. Many algorithms have been developed to mitigate
motion-dependent artifacts~\cite{vanderburg14}[CITE K2SC and EVEREST].
In \lightkurve, we implement an algorithm based off of the self-flat-field
(SFF) presented in~\cite{vanderburg14}.

Basically, SFF works by decorrelating the simple aperture flux
against the information on the spacecraft motion, obtained by computing the
arclength using the centroids of the target.

\begin{verbatim}
from lightkurve import KeplerTargetPixelFile
tpf = KeplerTargetPixelFile("ktwo248667471-c14_lpd-targ.fits.gz")
lc = tpf.to_lightcurve()
centroids = tpf.get_centroids()
lcc = lc.correct(centroids[0], centroids[1])
\end{verbatim}

%% If you wish to include an acknowledgments section in your paper,
%% separate it off from the body of the text using the \acknowledgments
%% command.
\acknowledgments

We would like to express our gratitude...

\vspace{5mm}
\facilities{Kepler}

\software{astropy}

%% Appendix material should be preceded with a single \appendix command.
%% There should be a \section command for each appendix. Mark appendix
%% subsections with the same markup you use in the main body of the paper.

%% Each Appendix (indicated with \section) will be lettered A, B, C, etc.
%% The equation counter will reset when it encounters the \appendix
%% command and will number appendix equations (A1), (A2), etc. The
%% Figure and Table counter will not reset.

\appendix

\section{Appendix information}

\bibliographystyle{aasjournal}
\bibliography{ms}

\end{document}
